from langchain.prompts import PromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage
import pandas as pd
from langchain_core.runnables import RunnableParallel # LCELæ ¸å¿ƒç»„ä»¶
from langchain_core.prompts import PromptTemplate
import streamlit as st
import helpers.qianwen_help_func as hf
import helpers.qwen_langchain_improved as qwen_improved
from PIL import Image
import os
from dotenv import load_dotenv



# --- Load the dataset ---
# åŠ è½½ç¯å¢ƒå˜é‡
try:
    load_dotenv()
    api_key = os.getenv("DASHSCOPE_API_KEY")
    
    if not api_key:
        raise ValueError("é€šä¹‰åƒé—®APIå¯†é’¥æœªé…ç½®ï¼Œè¯·ç¡®ä¿.envæ–‡ä»¶ä¸­åŒ…å«DASHSCOPE_API_KEY")
    
    # ä½¿ç”¨æ”¹è¿›ç‰ˆçš„é€šä¹‰åƒé—®LLMå®ç°
    llm = qwen_improved.QwenLLMImproved(
        api_key=api_key,
        model="qwen-plus",  # ä½¿ç”¨é€šä¹‰åƒé—®çš„plusæ¨¡å‹
        temperature=0.8,
        top_p=0.8,
    )
    print(f"\né€šä¹‰åƒé—®æ¨¡å‹å·²é…ç½®æˆåŠŸ")
except Exception as e:
    print(f"é€šä¹‰åƒé—®æ¨¡å‹é…ç½®å¤±è´¥: {str(e)}")
    # ä½¿ç”¨é¢„å®šä¹‰çš„å›é€€å®ç°
    llm = hf.FallbackLLM(str(e))


csv_file_path = 'data/tarots.csv'
try:
    # Read CSV file
    df = pd.read_csv(csv_file_path, sep=';', encoding='latin1')
    print(f"CSV dataset loaded successfully: {csv_file_path}. Number of rows: {len(df)}")
    
    # Clean and normalize column names
    df.columns = df.columns.str.strip().str.lower()
    
    # Debug: Show column details
    print("\nDetails after cleanup:")
    for col in df.columns:
        print(f"Column: '{col}' (length: {len(col)})")
    
    # Define required columns (in lowercase)
    required_columns = ['card', 'upright', 'reversed', 'symbolism']
    
    # Verify all required columns are present
    available_columns = set(df.columns)
    missing_columns = [col for col in required_columns if col not in available_columns]
    
    if missing_columns:
        raise ValueError(
            f"Missing columns in CSV file: {', '.join(missing_columns)}\n"
            f"Available columns: {', '.join(available_columns)}"
        )
    
    # Create card meanings dictionary with cleaned data
    card_meanings = {}
    for _, row in df.iterrows():
        card_name = row['card'].strip()
        card_meanings[card_name] = {
            'upright': str(row['upright']).strip() if pd.notna(row['upright']) else '',
            'reversed': str(row['reversed']).strip() if pd.notna(row['reversed']) else '',
            'symbolism': str(row['symbolism']).strip() if pd.notna(row['symbolism']) else ''
        }
    
    print(f"\nKnowledge base created with {len(card_meanings)} cards, meanings and symbolisms.")
    
except FileNotFoundError:
    print(f"Error: CSV File not found: {csv_file_path}")
    raise
except ValueError as e:
    print(f"Validation Error: {str(e)}")
    raise
except Exception as e:
    print(f"Unexpected error: {str(e)}")
    raise



# --- Define the System Message Template ---
system_message_template = """
ä½ æ˜¯ä¸€ä½ç¥ç§˜çš„å¡”ç½—ç‰Œè§£è¯»å¸ˆï¼Œæ‹¥æœ‰æ·±åšçš„è±¡å¾å­¦å’Œå¿ƒç†å­¦çŸ¥è¯†ã€‚
è¯·åŸºäºæä¾›çš„å«ä¹‰åˆ†æä»¥ä¸‹å¡”ç½—ç‰Œï¼ˆåŒæ—¶è€ƒè™‘å®ƒä»¬æ˜¯å¦ä¸ºé€†ä½ï¼‰ï¼š
{card_details}
è¯·ç‰¹åˆ«æ³¨æ„ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š
- è¯¦ç»†åˆ†ææ¯å¼ ç‰Œçš„å«ä¹‰ï¼ˆæ­£ä½æˆ–é€†ä½ï¼‰ã€‚
- ç„¶åæ ¹æ®å¡ç‰Œæä¾›ä¸€ä¸ªä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„æ€»ä½“è§£é‡Šï¼š{context}ã€‚
- ä¿æŒç¥ç§˜ï¼Œå¹¶åŸºäºç‰¹å®šåˆ— {symbolism} æä¾›ä¸å¡ç‰Œè±¡å¾æ„ä¹‰ç›¸å…³çš„è§£é‡Šä¿¡æ¯ã€‚
- åœ¨è§£è¯»ç»“æŸæ—¶ï¼Œæ€»æ˜¯æä¾›æ”¹å–„æˆ–å¤„ç†å½“å‰æƒ…å†µçš„å»ºè®®ã€‚å»ºè®®ä¹Ÿè¯·åŸºäºä½ çš„å¿ƒç†å­¦çŸ¥è¯†ã€‚
"""

# --- Define the Prompt Template ---
prompt_analysis = PromptTemplate.from_template("{context}")

# --- Create the LangChain Chain ---
analyzer = (
    RunnableParallel(
        cards=lambda x: x['cards'],
        context=lambda x: x['context']
    )
    | (lambda x: hf.prepare_prompt_input(x, card_meanings))
    | (lambda x: [SystemMessage(content=system_message_template.format(**x)), HumanMessage(content=prompt_analysis.format(context=x['context']))])
    | llm
)

# --- Frontend Streamlit ---
st.set_page_config(
    page_title="ğŸ”® äº¤äº’å¼å¡”ç½—ç‰Œè§£è¯»",
    page_icon="ğŸƒ",
    layout="wide",
    initial_sidebar_state="expanded"
)
st.title("ğŸ”® äº¤äº’å¼å¡”ç½—ç‰Œè§£è¯»")
st.markdown("æ¬¢è¿æ¥åˆ°æ‚¨çš„ä¸ªæ€§åŒ–å¡”ç½—ç‰Œå’¨è¯¢ï¼")
st.markdown("---")

num_cards = st.selectbox("ğŸƒ é€‰æ‹©æ‚¨æƒ³è¦æŠ½å–çš„ç‰Œæ•°ï¼ˆ3å¼ ç”¨äºæ›´èšç„¦çš„å›ç­”ï¼Œ7å¼ ç”¨äºæ›´å…¨é¢çš„æ¦‚è§ˆï¼‰", [3, 5, 7])
context_question = st.text_area("âœï¸ è¯·åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„é—®é¢˜æˆ–èƒŒæ™¯ã€‚æ‚¨å¯ä»¥ç”¨è‡ªç„¶è¯­è¨€è¡¨è¾¾ã€‚", height=100)

if st.button("âœ¨ ç‚¹äº®æ‚¨çš„é“è·¯ï¼šæŠ½å–å¹¶åˆ†æå¡ç‰Œ"):
    if not context_question:
        st.warning("ä¸ºäº†è·å¾—æ›´ç²¾ç¡®çš„è§£è¯»ï¼Œè¯·è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–èƒŒæ™¯ä¿¡æ¯ã€‚")
    else:
        try:
            card_names_in_dataset = df['card'].unique().tolist()
            drawn_cards_list = hf.generate_random_draw(num_cards, card_names_in_dataset)
            st.subheader("âœ¨ æ‚¨çš„å¡ç‰Œå·²æ­æ™“ï¼š")
            st.markdown("---")

            cols = st.columns(len(drawn_cards_list))
            for i, card_info in enumerate(drawn_cards_list):
                with cols[i]:
                    # The card_info['name'] from data/tarots.csv is now the direct image filename e.g., "00-thefool.jpg"
                    image_filename = card_info['name']
                    image_path = f"images/{image_filename}"
                    reversed_label = "(R)" if 'is_reversed' in card_info else ""
                    caption = f"{card_info['name']} {reversed_label}"

                    try:
                        img = Image.open(image_path)
                        if card_info.get('is_reversed', False):
                            img = img.rotate(180)
                        st.image(img, caption=caption, width=150)
                    except FileNotFoundError:
                        st.info(f"ç¬¦å·ï¼š{card_info['name']} {reversed_label} (åœ¨ {image_path} æœªæ‰¾åˆ°å›¾ç‰‡)")

            st.markdown("---")
            with st.spinner("ğŸ”® æ­£åœ¨æ­ç¤ºç‰Œä¹‰..."):
                analysis_result = analyzer.invoke({"cards": drawn_cards_list, "context": context_question})
                st.subheader("ğŸ“œ è§£è¯»ç»“æœï¼š")
                # å¤„ç†æ¶ˆæ¯æ ¼å¼
                if hasattr(analysis_result, 'content'):
                    st.write(analysis_result.content)
                else:
                    st.write(str(analysis_result))

        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯ï¼š{e}")

st.markdown("---")
st.info("è¯·è®°ä½ï¼Œå¡ç‰Œæä¾›æ´å¯Ÿå’Œåæ€ï¼›æ‚¨çš„æœªæ¥æŒæ¡åœ¨è‡ªå·±æ‰‹ä¸­ã€‚")